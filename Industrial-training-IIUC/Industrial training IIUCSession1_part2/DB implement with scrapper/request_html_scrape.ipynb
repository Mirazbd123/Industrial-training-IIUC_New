{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL Database connection successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n",
      "Query successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from dotenv import load_dotenv\n",
    "from database_connection import create_db_connection\n",
    "from requests_html import HTMLSession\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from news_insert import (execute_query,\n",
    "                        insert_reporter, \n",
    "                        insert_category, \n",
    "                        insert_news,\n",
    "                        insert_publisher,\n",
    "                        insert_image,\n",
    "                        insert_summary\n",
    "                        )\n",
    "\n",
    "dictionary = {}\n",
    "dictionary['category'] = []\n",
    "dictionary['title'] = []\n",
    "dictionary['time & author'] = []\n",
    "dictionary['body'] = []\n",
    "dictionary['image_link'] = []\n",
    "dictionary['page_link'] = []\n",
    "\n",
    "def process_and_insert_news_data(connection,category, title, body, image_link, page_link, author,\n",
    "       time_date, category_description, reporter_mail, publisher_name,\n",
    "       publisher_email, publisher_phone, head_office_address, website ):\n",
    "    \n",
    "    try:\n",
    "        # Insert category if not exists\n",
    "        category_id = insert_category(connection, category, category_description)\n",
    "        \n",
    "        # Insert reporter if not exists\n",
    "        reporter_id = insert_reporter(connection, author, reporter_mail)\n",
    "        \n",
    "        # Insert publisher as a placeholder (assuming publisher is not provided)\n",
    "        publisher_id = insert_publisher(connection, publisher_name, publisher_email,publisher_phone,head_office_address,website,\n",
    "                                        \"facebook.com/dailynayadigonto\" , \"twitter.com/dailynayadigonto\" , \n",
    "                                        \"linkedin.com/dailynayadigonto\" , \"instagram.com/dailynayadigonto\")\n",
    "        \n",
    "        # Insert news article\n",
    "        news_id = insert_news(connection, category_id, reporter_id, publisher_id, time_date, title, body, page_link)\n",
    "        \n",
    "        # Insert images\n",
    "        image_id = insert_image(connection, news_id, image_link)\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error while processing news data - {e}\")\n",
    "\n",
    "\n",
    "def render_javascript(url):\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        # response.html.render()  # This will download Chromium if not found\n",
    "        print(\"Rendered web page:\", response.html.html)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "def extract_title_link(url): #use only once\n",
    "    session = HTMLSession()\n",
    "    try:\n",
    "        link_lists = []\n",
    "        response = session.get(url)\n",
    "        title_link_lead = response.html.find(\"div.news-caption-lead > h2 > a\")\n",
    "        link_lists.append(title_link_lead[0].attrs['href'])\n",
    "\n",
    "        title_links = response.html.find(\"div.news-caption > h2 > a\")\n",
    "        for link in title_links:\n",
    "            link_lists.append(link.attrs['href'])\n",
    "\n",
    "        news_title_links = response.html.find(\"div.news-title > h3 > a\")\n",
    "        for link in news_title_links:\n",
    "            link_lists.append(link.attrs['href'])\n",
    "        \n",
    "        return link_lists\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "def extract_information(url):\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        dictionary['page_link'].append(url)\n",
    "\n",
    "        # Example: Extracting category\n",
    "        category = response.html.find(\"ol.breadcrumb > li > a\")\n",
    "        # for link in category:\n",
    "        #     print(f\"Category : {link.text} \")\n",
    "        # print(f\"Category : {category[2].text}\\n\")\n",
    "        dictionary['category'].append(category[2].text)\n",
    "\n",
    "        #Extracting Title\n",
    "        title = response.html.find(\"h1.headline\")\n",
    "        # print(f\"Title : {title[0].text}\\n\")\n",
    "        dictionary['title'].append(title[0].text)\n",
    "\n",
    "        #Extracting Time and author\n",
    "        times = response.html.find(\"div.col-md-6 > ul.list-inline\")\n",
    "        # for i in times:\n",
    "        #     # new_time = i.html.find('li')\n",
    "        #     print(i.text)\n",
    "        # print(f\"Time and author : {times[2].text}\\n\")\n",
    "        # print(len(times))\n",
    "        dictionary['time & author'].append(times[2].text)\n",
    "\n",
    "        #Extracting body\n",
    "        body = response.html.find(\"div.news-content > p\")\n",
    "        # print(\"Body : \")\n",
    "        bodyy = ''\n",
    "        for i in body:\n",
    "            # print(i.text)\n",
    "            bodyy += i.text\n",
    "        # print(\"\\n\")\n",
    "        dictionary['body'].append(bodyy)\n",
    "\n",
    "        #extracting Image source\n",
    "        img = response.html.find(\"div.image-holder > figure.figure > img.img-responsive\")\n",
    "        # print(f\"Image Link : {img[0].attrs['src']}\\n\")\n",
    "        dictionary['image_link'].append(img[0].attrs['src'])\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "def AuthorSplit(text):\n",
    "    author = text.split(\"\\n\")\n",
    "    return author[0].strip()\n",
    "\n",
    "def TimeSplit(text):\n",
    "    author = text.split(\"\\n\")\n",
    "    return author[1].strip()\n",
    "\n",
    "def update_trim(text):\n",
    "    if text.find(\"আপডেট:\")!=-1:\n",
    "        return text.split(\"আপডেট: \")[1].strip()\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conn = create_db_connection()\n",
    "    link_lists = extract_title_link(\"https://www.dailynayadiganta.com/\")\n",
    "    for i in link_lists:\n",
    "        extract_information(i)\n",
    "    df = pd.DataFrame.from_dict(dictionary)\n",
    "    # Data preprocessing\n",
    "    df['author'] = df['time & author'].apply(AuthorSplit)\n",
    "    df['time_date'] = df['time & author'].apply(TimeSplit)\n",
    "    df.drop(columns='time & author' , inplace=True)\n",
    "    df['time_date'] = df['time_date'].apply(update_trim)\n",
    "    df['category_description'] = \"All news regarding \" + df['category']\n",
    "    df['reporter_mail'] = df['author']+\"@gmail.com\"\n",
    "    df['publisher_name'] = \"নয়া দিগন্ত\"\n",
    "    df['publisher_email'] = \"info@nayadigonto.com\"\n",
    "    df['publisher_phone'] = \"৫৭১৬৫২৬১-৯\"\n",
    "    df['head_office_address'] = \"১ আর. কে মিশন রোড, (মানিক মিয়া ফাউন্ডেশন ভবন) , ঢাকা-১২০৩\"\n",
    "    df['website'] = \"https://www.dailynayadiganta.com\"\n",
    "\n",
    "    #insert into database\n",
    "    lenth = df.shape[0]\n",
    "    for i in range (lenth):\n",
    "        process_and_insert_news_data(conn,df['category'][i], df['title'][i],\n",
    "                                  df['body'][i], df['image_link'][i], df['page_link'][i],df['author'][i],df['time_date'][i], \n",
    "                                  df['category_description'][i], df['reporter_mail'][i], df['publisher_name'][i],df['publisher_email'][i], \n",
    "                                  df['publisher_phone'][i], df['head_office_address'][i], df['website'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
